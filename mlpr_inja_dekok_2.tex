\documentclass[a4paper,11pt]{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}

\definecolor{gray}{rgb}{0.5,0.5,0.5}
\newcommand{\convolution}{\ensuremath{+\negmedspace\negmedspace\negmedspace\times}}

\author{Maarten Inja \and Patrick de Kok}
\title{MLPR: Lab 1}

\lstset{
  language=Octave,
  basicstyle=\footnotesize,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=5,
  showspaces=false,
  showstringspaces=true
  showtabs=false,
  frame=single,
  title=\lstname,
  breaklines=true,
}
\begin{document}
\maketitle

\section*{Exercise 1}
\begin{figure}[h]
  \caption{A plot of the dataset.  The red ``$\color{red}+$'' and magenta ``$\color{magenta}\circ$'' represent the training and test data points of class A.  The blue ``$\color{blue}\times$'' and black ``$\convolution$'' represent the training and test data points of class B.}
  \label{plot1}
  \begin{center}
    \includegraphics[width=0.6\paperwidth]{plot1}
  \end{center}
\end{figure}

Our training and test data sets are generated through the Matlab code presented in \autoref{code}.  A graphical representation is given in \autoref{plot1}.

\lstinputlisting[caption={The code which generates our data set},label=code]{preprocess_data.m}

\section*{Exercise 2}
\label{sec:two}
Confusion matrices of the KNN with $1 \leq k \leq 20$:

\begin{table}
  \caption{TA = True A; TB = True B; CA = classified as A; CB = classified as B.}
\begin{tabular}{lcrr}
$k$ && CA & CB \\ \\
 1 & TA & 199 &    51 \\
& TB &  34 &   216 \\
 2 & TA & 205 &    45 \\
& TB &  34 &   216 \\
 3 & TA & 212 &    38 \\
& TB &  33 &   217 \\
 4 & TA & 217 &    33 \\
& TB &  35 &   215 \\
 5 & TA & 214 &    36 \\
& TB &  30 &   220 \\
 6 & TA & 217 &    33 \\
& TB &  31 &   219 \\
 7 & TA & 216 &    34 \\
& TB &  31 &   219 \\
 8 & TA & 221 &    29 \\
& TB &  32 &   218 \\
 9 & TA & 216 &    34 \\
& TB &  30 &   220 \\
 10 & TA & 217 &    33 \\
& TB &  32 &   218 \\
 11 & TA & 214 &    36 \\
& TB &  29 &   221 \\
 12 & TA & 215 &    35 \\
& TB &  28 &   222 \\
 13 & TA & 215 &    35 \\
& TB &  31 &   219 \\
 14 & TA & 216 &    34 \\
& TB &  31 &   219 \\
 15 & TA & 216 &    34 \\
& TB &  31 &   219 \\
 16 & TA & 213 &    37 \\
& TB &  31 &   219 \\
 17 & TA & 215 &    35 \\
& TB &  35 &   215 \\
 18 & TA & 215 &    35 \\
& TB &  34 &   216 \\
 19 & TA & 215 &    35 \\
& TB &  36 &   214 \\
 20 & TA & 216 &    34 \\
& TB &  33 &   217 \\
\end{tabular}
\end{table}

\section*{Exercise 3}
\begin{enumerate}
    \item \textit{Discuss briefly (200 words) what the advantage would be of using a method like cross-validation or
bootstrapping in this context.}

The method used in section \ref{sec:two} biases the machine towards the test set. The test set should normally be
used to evaluate the machine learning algorithm and/or its parameters. The test set should not be used to 
select a model, as was implied in section \ref{sec:two}. 
Cross-validation can be used to fairly evaluate models using solely the training set without the risk of 
introducing a bias for one specific validation set. 

Bootstrapping 

\item \textit{Implement 10-fold cross-validation, and give an estimate for a good value of k.}

\item \textit{Can you use this estimate of k in your ﬁnal classiﬁer? What is the use of doing cross-validation?}

\item \textit{ How do you compute the test error? What is it an estimate of? What would be the use of a validation
set? }



\end{enumerate}

\end{document}
