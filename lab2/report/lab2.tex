\documentclass[a4paper,11pt]{article}
\usepackage{amsmath}
%\usepackage{catoptions}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}

\definecolor{gray}{rgb}{0.5,0.5,0.5}
\newcommand{\convolution}{\ensuremath{+\negmedspace\negmedspace\negmedspace\times}}
\newcommand{\highlightColor}{red}

\newcommand{\todo}[1]{\colorbox{red}{\color{white}#1}}

\makeatletter
%% Provide \Autoref; the \autoref with a printed capital
\def\figureautorefname{figure}
\def\tableautorefname{table}
\def\partautorefname{part}
\def\appendixautorefname{appendix}
\def\equationautorefname{equation}
\def\AMSautorefname{equation}
\def\theoremautorefname{theorem}
\def\enumerationautorefname{case}
\def\Autoref#1{%
  \begingroup
  \edef\reserved@a{\cpttrimspaces{#1}}%
  \ifcsndefTF{r@#1}{%
    \xaftercsname{\expandafter\testreftype\@fourthoffive}
      {r@\reserved@a}.\\{#1}%
  }{%
    \ref{#1}%
  }%
  \endgroup
}
\def\testreftype#1.#2\\#3{%
  \ifcsndefTF{#1autorefname}{%
    \def\reserved@a##1##2\@nil{%
      \uppercase{\def\ref@name{##1}}%
      \csn@edef{#1autorefname}{\ref@name##2}%
      \autoref{#3}%
    }%
    \reserved@a#1\@nil
  }{%
    \autoref{#3}%
  }%
}


\makeatother

 
\author{Maarten Inja (5872464) \and Patrick de Kok (5640318)}
\title{MLPR: Lab 2: Naive Bayes SPAM detector}


\lstset{
  language=Octave,
  basicstyle=\footnotesize,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=5,
  showspaces=false,
  showstringspaces=true
  showtabs=false,
  frame=single,
  title=\lstname,
  breaklines=true,
}
\begin{document}
\maketitle

In this report we describe how we have build a naive Bayes classifier, which will classify text messages as either ``spam'' (unwanted messages), or ham (wanted messages).  

For the implementation we have used Python (version 2.7) with standard libraries.  We do not have a copy of Matlab on our PCs, and thus wanted to run the provided code on Octave, an free and open implementation of Matlab.  The provided code proved not to be compatible with the version of Octave included in our Linux repositories.  Although we have upgraded our Octave version from 3.2 to 3.6, as well as the I/O libraries, the code did not want to run.  After talking with Gwenn Englebienne, we decided to implement the algorithm in Python. 

\section{Feature Selection}
The feature vector $\vec{x}$ consists of those words (expressed as 
regular expression) which 
help to distinguish between spam and ham emails the most.

The naive Bayes classifier uses only words as features. This implies that we should pick these words carefully and correctly. More advanced spam classifiers would use other features as well, such as those that can be collected from the header of an email.

We compute the most characterizing features by maximizing the difference between $p(SPAM, w)$ and $p(HAM, w)$ for each occurring word $w$.  The text is filtered and split on on-alphabetical characters to get a list of words.

For a class $C$, we compute the probability $p(C, w)$ in the following way:
\begin{align*}
  p(C, w) 
    &= p(w | C) p(C) \\
    &= \frac{\#(w, C) p(C)}{\sum_{w'} \#(w', C)}
\end{align*}

Here we use $\#(w, C)$ to denote the number of occurrences of a word $w$ in the file collection of class $C$.  We divide this by the amount of all words in the files of that class.

We then sort the words based on the difference in probability $|p(SPAM, w) - p(HAM, w)|$, and select the 300 highest-ranking words.

This number of features is selected arbitrarily; we have not inspected if the classifier works better with a higher or lower amount of features.

For the class prior $p(C)$, we have taken $p(SPAM) = 0.888$ and $p(HAM) = 1 - p(SPAM) = 0.112$.  These numbers come from the report of the Messaging Anti-Abuse Work Group's statistics, collected over the third quarter of 2011 \footnote{The report can be found at \url{http://www.maawg.org/sites/maawg/files/news/MAAWG_2011_Q1Q2Q3_Metrics_Report_15.pdf}.}.  We have not found any clue of when the data has been collected, but we assumed that it would be fairly recently.  These values are the most recent the MAAWG has published, and we hope they will make do.

% Dit moeten we even opnieuw bekijken {{{
We then apply some common sense to pick those words that seem logically to 
pick (i.e. not `his', but rather `pills').  

Below are the first few lines of the output of \texttt{python features.py}, 
notice how it also prints some more information from the data from which we 
are inclined to conclude that SPAM mostly consist of emails with fewer words.  We have not used this as a feature, as it was specified in the assignment to look only at the occurring words.
\begin{center}
  % TODO: Dit moeten we even vervangen.
\begin{verbatim}
spam files: 182 (44.61%)
ham files: 226 (55.39%)

spam words: 18927 (25.08%)
ham words: 56552 (74.92%)

Unique words: 17766

SPAM
'subject',      0.000280
'meds',         0.000179
'your',         0.000160
'my',           0.000141
'pills',        0.000112
'funds',        0.000107
'transfer',     0.000106
'make',         0.000106
'his',          0.000105
\end{verbatim}
\end{center}

We experimented with some features, and also investigated some of the 
messages. The resulting feature vector was: 
\['subject', 'meds', 'pills', 'funds', 'transfer', 'make', 'save', 'viagra', 'replica', 'http', 'www'\].
% }}}

\todo{insert here}

\section{Naive Bayes Classifier}
The Naive Bayes Classifier works with the following equations: 

\begin{equation}
P(C_k|x) = \frac{P(x|C_k)p(C_k)}{\sum_j P(x|C_j)P(C_j)}
\label{eq:1}
\end{equation}

\begin{equation}
P(x|C_k)= \prod_{i=1}^d \mu^{x_i}_{i, k}(1 - \mu_{i,k})^{1-x_i}
\label{eq:2}
\end{equation}

These are implemented in \texttt{bayes.py}. 

Laplace smoothing with $\alpha = 1$ was used to make sure one zero occurrence did not made the
product for the entire probability zero as well. 

% $\mu^{i,k}$ is the chance for word $x_i$ given class $C_k$ and can 
% easily be calculated by dividing the number of emails in which 
% $x_i$ occurs by the total amount of emails in $C-k$.  \ref{eq:2}

\section{Evaluation}

\end{document}
